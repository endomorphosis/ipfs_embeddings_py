{
  "summary": {
    "passed": 3,
    "total": 8,
    "success_rate": 37.5,
    "duration": 46.349549531936646,
    "timestamp": 1749280553.7897413
  },
  "results": {
    "mcp_tools": {
      "status": "failed",
      "details": {
        "returncode": 1,
        "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.0, pluggy-1.6.0 -- /home/barberb/laion-embeddings-1/.venv/bin/python\ncachedir: .pytest_cache\nbenchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\nrootdir: /home/barberb/laion-embeddings-1\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.1.1, benchmark-5.1.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 18 items\n\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_core_imports PASSED [  5%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_authentication_tool_instantiation PASSED [ 11%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_token_validation_tool_instantiation PASSED [ 16%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_session_manager_instantiation PASSED [ 22%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_rate_limit_tool_instantiation PASSED [ 27%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_vector_store_tool_with_parameters FAILED [ 33%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_mcp_server_enhanced_import PASSED [ 38%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_project_structure_integrity PASSED [ 44%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_critical_files_exist PASSED [ 50%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_all_tools_listed ERROR [ 55%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_tools_have_valid_schemas ERROR [ 61%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_mcp_protocol_methods ERROR [ 66%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_tool_execution_basic ERROR [ 72%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_error_handling ERROR [ 77%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_tool_registry_functionality FAILED [ 83%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_expected_tool_count FAILED [ 88%]\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsIntegration::test_mcp_server_cli_validation FAILED [ 94%]\ntest/test_mcp_tools_comprehensive.py::test_mcp_tools_comprehensive FAILED [100%]\n\n==================================== ERRORS ====================================\n______ ERROR at setup of TestMCPToolsComprehensive.test_all_tools_listed _______\nfile /home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py, line 133\n      @pytest.mark.asyncio\n      async def test_all_tools_listed(self, mcp_server):\n          \"\"\"Test all expected tools are available\"\"\"\n          expected_tool_patterns = [\n              \"create_embeddings\",\n              \"search_embeddings\",\n              \"shard_embeddings\",\n              \"sparse_embeddings\",\n              \"ipfs_\",\n              \"vector_store\",\n              \"cache_\",\n              \"monitoring_\",\n              \"session_\",\n              \"auth_\",\n              \"admin_\",\n              \"workflow_\",\n              \"background_task\",\n              \"rate_limiting\",\n              \"data_processing\"\n          ]\n\n          available_tools = list(mcp_server.tools.keys())\n          logger.info(f\"Available tools: {available_tools}\")\n\n          # Check that we have tools from major categories\n          found_patterns = 0\n          for pattern in expected_tool_patterns:\n              if any(pattern in tool_name for tool_name in available_tools):\n                  found_patterns += 1\n\n          # Should find most pattern categories\n          assert found_patterns >= len(expected_tool_patterns) * 0.6, \\\n              f\"Missing tool categories. Found {found_patterns}/{len(expected_tool_patterns)} patterns\"\nE       fixture 'mcp_server' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_problematic_imports, monkeypatch, no_cover, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_metadata, sample_texts, sample_vectors, setup_testing_environment, temp_directory, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py:133\n__ ERROR at setup of TestMCPToolsComprehensive.test_tools_have_valid_schemas ___\nfile /home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py, line 167\n      @pytest.mark.asyncio\n      async def test_tools_have_valid_schemas(self, mcp_server):\n          \"\"\"Test all tools have valid input schemas\"\"\"\n          for tool_name, tool_data in mcp_server.tools.items():\n              # Each tool should have description and parameters\n              assert \"description\" in tool_data, f\"Tool {tool_name} missing description\"\n              assert \"parameters\" in tool_data, f\"Tool {tool_name} missing parameters\"\n              assert \"instance\" in tool_data, f\"Tool {tool_name} missing instance\"\n\n              # Description should be non-empty\n              assert tool_data[\"description\"], f\"Tool {tool_name} has empty description\"\n\n              # Parameters should be a dict (JSON schema)\n              assert isinstance(tool_data[\"parameters\"], dict), \\\n                  f\"Tool {tool_name} parameters not a dict\"\nE       fixture 'mcp_server' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_problematic_imports, monkeypatch, no_cover, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_metadata, sample_texts, sample_vectors, setup_testing_environment, temp_directory, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py:167\n____ ERROR at setup of TestMCPToolsComprehensive.test_mcp_protocol_methods _____\nfile /home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py, line 183\n      @pytest.mark.asyncio\n      async def test_mcp_protocol_methods(self, mcp_server):\n          \"\"\"Test MCP protocol method handling\"\"\"\n          # Test initialize method\n          init_request = {\n              \"method\": \"initialize\",\n              \"params\": {\"protocolVersion\": \"2024-11-05\"}\n          }\n\n          response = await mcp_server.handle_request(init_request)\n          assert \"result\" in response, \"Initialize should return result\"\n          assert \"protocolVersion\" in response[\"result\"]\n          assert \"capabilities\" in response[\"result\"]\n          assert \"serverInfo\" in response[\"result\"]\n\n          # Test tools/list method\n          list_request = {\"method\": \"tools/list\", \"params\": {}}\n          response = await mcp_server.handle_request(list_request)\n          assert \"result\" in response, \"Tools list should return result\"\n          assert \"tools\" in response[\"result\"]\n          assert isinstance(response[\"result\"][\"tools\"], list)\n\n          # Test validation/status method\n          status_request = {\"method\": \"validation/status\", \"params\": {}}\n          response = await mcp_server.handle_request(status_request)\n          assert \"result\" in response, \"Validation status should return result\"\n          assert \"validation\" in response[\"result\"]\n          assert \"tools_count\" in response[\"result\"]\nE       fixture 'mcp_server' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_problematic_imports, monkeypatch, no_cover, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_metadata, sample_texts, sample_vectors, setup_testing_environment, temp_directory, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py:183\n____ ERROR at setup of TestMCPToolsComprehensive.test_tool_execution_basic _____\nfile /home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py, line 212\n      @pytest.mark.asyncio\n      async def test_tool_execution_basic(self, mcp_server):\n          \"\"\"Test basic tool execution (non-destructive tools only)\"\"\"\n          safe_tools_to_test = []\n\n          # Find safe tools that can be tested without side effects\n          for tool_name in mcp_server.tools.keys():\n              if any(safe_pattern in tool_name.lower() for safe_pattern in\n                     [\"list\", \"get\", \"status\", \"info\", \"check\", \"validate\"]):\n                  safe_tools_to_test.append(tool_name)\n\n          if not safe_tools_to_test:\n              logger.warning(\"No safe tools found for execution testing\")\n              return\n\n          # Test a few safe tools\n          for tool_name in safe_tools_to_test[:3]:  # Test first 3 safe tools\n              try:\n                  call_request = {\n                      \"method\": \"tools/call\",\n                      \"params\": {\n                          \"name\": tool_name,\n                          \"arguments\": {}  # Empty args for basic test\n                      }\n                  }\n\n                  response = await mcp_server.handle_request(call_request)\n\n                  # Tool should either succeed or fail gracefully\n                  assert \"result\" in response or \"error\" in response, \\\n                      f\"Tool {tool_name} returned invalid response format\"\n\n                  if \"result\" in response:\n                      logger.info(f\"\u2705 Tool {tool_name} executed successfully\")\n                  else:\n                      logger.info(f\"\u26a0\ufe0f Tool {tool_name} failed gracefully: {response.get('error', {}).get('message', 'Unknown error')}\")\n\n              except Exception as e:\n                  logger.warning(f\"Tool {tool_name} execution test failed: {str(e)}\")\nE       fixture 'mcp_server' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_problematic_imports, monkeypatch, no_cover, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_metadata, sample_texts, sample_vectors, setup_testing_environment, temp_directory, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py:212\n_______ ERROR at setup of TestMCPToolsComprehensive.test_error_handling ________\nfile /home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py, line 252\n      @pytest.mark.asyncio\n      async def test_error_handling(self, mcp_server):\n          \"\"\"Test error handling for invalid requests\"\"\"\n          # Test unknown method\n          unknown_request = {\"method\": \"unknown_method\", \"params\": {}}\n          response = await mcp_server.handle_request(unknown_request)\n          assert \"error\" in response, \"Unknown method should return error\"\n\n          # Test invalid tool call\n          invalid_tool_request = {\n              \"method\": \"tools/call\",\n              \"params\": {\"name\": \"nonexistent_tool\", \"arguments\": {}}\n          }\n          response = await mcp_server.handle_request(invalid_tool_request)\n          assert \"error\" in response, \"Nonexistent tool should return error\"\nE       fixture 'mcp_server' not found\n>       available fixtures: _class_event_loop, _function_event_loop, _module_event_loop, _package_event_loop, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_problematic_imports, monkeypatch, no_cover, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_metadata, sample_texts, sample_vectors, setup_testing_environment, temp_directory, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/barberb/laion-embeddings-1/test/test_mcp_tools_comprehensive.py:252\n=================================== FAILURES ===================================\n_______ TestMCPToolsComprehensive.test_vector_store_tool_with_parameters _______\nasync def functions are not natively supported.\nYou need to install a suitable plugin for your async framework, for example:\n  - anyio\n  - pytest-asyncio\n  - pytest-tornasync\n  - pytest-trio\n  - pytest-twisted\n__________ TestMCPToolsComprehensive.test_tool_registry_functionality __________\ntest/test_mcp_tools_comprehensive.py:272: in test_tool_registry_functionality\n    from ipfs_embeddings_py.ipfs_embeddings import ipfs_embeddings_py\nipfs_embeddings_py/__init__.py:1: in <module>\n    from .ipfs_embeddings import ipfs_embeddings_py\nipfs_embeddings_py/ipfs_embeddings.py:5: in <module>\n    import datasets\n.venv/lib/python3.12/site-packages/datasets/__init__.py:17: in <module>\n    from .arrow_dataset import Dataset\n.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:72: in <module>\n    from . import config\n.venv/lib/python3.12/site-packages/datasets/config.py:143: in <module>\n    TORCHVISION_AVAILABLE = importlib.util.find_spec(\"torchvision\") is not None\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n<frozen importlib.util>:109: in find_spec\n    ???\nE   ValueError: torchvision.__spec__ is not set\n\nDuring handling of the above exception, another exception occurred:\ntest/test_mcp_tools_comprehensive.py:296: in test_tool_registry_functionality\n    pytest.fail(f\"Tool registry test failed: {str(e)}\")\nE   Failed: Tool registry test failed: torchvision.__spec__ is not set\n______________ TestMCPToolsComprehensive.test_expected_tool_count ______________\ntest/test_mcp_tools_comprehensive.py:302: in test_expected_tool_count\n    from ipfs_embeddings_py.ipfs_embeddings import ipfs_embeddings_py\nipfs_embeddings_py/__init__.py:1: in <module>\n    from .ipfs_embeddings import ipfs_embeddings_py\nipfs_embeddings_py/ipfs_embeddings.py:5: in <module>\n    import datasets\n.venv/lib/python3.12/site-packages/datasets/__init__.py:17: in <module>\n    from .arrow_dataset import Dataset\n.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:72: in <module>\n    from . import config\n.venv/lib/python3.12/site-packages/datasets/config.py:143: in <module>\n    TORCHVISION_AVAILABLE = importlib.util.find_spec(\"torchvision\") is not None\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n<frozen importlib.util>:109: in find_spec\n    ???\nE   ValueError: torchvision.__spec__ is not set\n\nDuring handling of the above exception, another exception occurred:\ntest/test_mcp_tools_comprehensive.py:324: in test_expected_tool_count\n    pytest.fail(f\"Tool count test failed: {str(e)}\")\nE   Failed: Tool count test failed: torchvision.__spec__ is not set\n____________ TestMCPToolsIntegration.test_mcp_server_cli_validation ____________\ntest/test_mcp_tools_comprehensive.py:352: in test_mcp_server_cli_validation\n    pytest.fail(f\"MCP server validation failed: {result.stderr}\")\nE   Failed: MCP server validation failed: 2025-06-07 00:15:25,470 - __main__ - INFO - \ud83d\udd25 Starting LAION Embeddings MCP Server...\nE   2025-06-07 00:15:25,470 - __main__ - INFO - \ud83d\ude80 Initializing LAION Embeddings MCP Server...\nE   2025-06-07 00:15:26,512 - datasets - INFO - PyTorch version 2.7.1 available.\nE   2025-06-07 00:15:29,457 - faiss.loader - INFO - Loading faiss with AVX2 support.\nE   2025-06-07 00:15:29,484 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\nE   2025-06-07 00:15:29,490 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\nE   2025-06-07 00:15:40,985 - __main__ - ERROR - \u274c Failed to initialize MCP server: No module named 'ipfs_kit_py.ipfs_kit'\nE   2025-06-07 00:15:40,986 - __main__ - ERROR - Traceback (most recent call last):\nE     File \"/home/barberb/laion-embeddings-1/mcp_server.py\", line 46, in initialize\nE       from ipfs_embeddings_py.ipfs_embeddings import ipfs_embeddings_py\nE     File \"/home/barberb/laion-embeddings-1/ipfs_embeddings_py/__init__.py\", line 1, in <module>\nE       from .ipfs_embeddings import ipfs_embeddings_py\nE     File \"/home/barberb/laion-embeddings-1/ipfs_embeddings_py/ipfs_embeddings.py\", line 98, in <module>\nE       from ipfs_kit_py.ipfs_kit import ipfs_kit\nE   ModuleNotFoundError: No module named 'ipfs_kit_py.ipfs_kit'\nE   \nE   2025-06-07 00:15:40,986 - __main__ - ERROR - \u274c Server initialization failed\n------------------------------ Captured log call -------------------------------\nERROR    test.test_mcp_tools_comprehensive:test_mcp_tools_comprehensive.py:349 MCP server validation failed with code 1\nERROR    test.test_mcp_tools_comprehensive:test_mcp_tools_comprehensive.py:350 STDERR: 2025-06-07 00:15:25,470 - __main__ - INFO - \ud83d\udd25 Starting LAION Embeddings MCP Server...\n2025-06-07 00:15:25,470 - __main__ - INFO - \ud83d\ude80 Initializing LAION Embeddings MCP Server...\n2025-06-07 00:15:26,512 - datasets - INFO - PyTorch version 2.7.1 available.\n2025-06-07 00:15:29,457 - faiss.loader - INFO - Loading faiss with AVX2 support.\n2025-06-07 00:15:29,484 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n2025-06-07 00:15:29,490 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n2025-06-07 00:15:40,985 - __main__ - ERROR - \u274c Failed to initialize MCP server: No module named 'ipfs_kit_py.ipfs_kit'\n2025-06-07 00:15:40,986 - __main__ - ERROR - Traceback (most recent call last):\n  File \"/home/barberb/laion-embeddings-1/mcp_server.py\", line 46, in initialize\n    from ipfs_embeddings_py.ipfs_embeddings import ipfs_embeddings_py\n  File \"/home/barberb/laion-embeddings-1/ipfs_embeddings_py/__init__.py\", line 1, in <module>\n    from .ipfs_embeddings import ipfs_embeddings_py\n  File \"/home/barberb/laion-embeddings-1/ipfs_embeddings_py/ipfs_embeddings.py\", line 98, in <module>\n    from ipfs_kit_py.ipfs_kit import ipfs_kit\nModuleNotFoundError: No module named 'ipfs_kit_py.ipfs_kit'\n\n2025-06-07 00:15:40,986 - __main__ - ERROR - \u274c Server initialization failed\n\nERROR    test.test_mcp_tools_comprehensive:test_mcp_tools_comprehensive.py:351 STDOUT:\n_________________________ test_mcp_tools_comprehensive _________________________\ntest/test_mcp_tools_comprehensive.py:375: in test_mcp_tools_comprehensive\n    asyncio.run(run_async_tests())\n/usr/lib/python3.12/asyncio/runners.py:194: in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n/usr/lib/python3.12/asyncio/runners.py:118: in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/usr/lib/python3.12/asyncio/base_events.py:687: in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\ntest/test_mcp_tools_comprehensive.py:385: in run_async_tests\n    raise Exception(\"MCP server initialization failed\")\nE   Exception: MCP server initialization failed\n------------------------------ Captured log call -------------------------------\nERROR    mcp_server:mcp_server.py:113 \u274c Failed to initialize MCP server: torchvision.__spec__ is not set\nERROR    mcp_server:mcp_server.py:114 Traceback (most recent call last):\n  File \"/home/barberb/laion-embeddings-1/mcp_server.py\", line 46, in initialize\n    from ipfs_embeddings_py.ipfs_embeddings import ipfs_embeddings_py\n  File \"/home/barberb/laion-embeddings-1/ipfs_embeddings_py/__init__.py\", line 1, in <module>\n    from .ipfs_embeddings import ipfs_embeddings_py\n  File \"/home/barberb/laion-embeddings-1/ipfs_embeddings_py/ipfs_embeddings.py\", line 5, in <module>\n    import datasets\n  File \"/home/barberb/laion-embeddings-1/.venv/lib/python3.12/site-packages/datasets/__init__.py\", line 17, in <module>\n    from .arrow_dataset import Dataset\n  File \"/home/barberb/laion-embeddings-1/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 72, in <module>\n    from . import config\n  File \"/home/barberb/laion-embeddings-1/.venv/lib/python3.12/site-packages/datasets/config.py\", line 143, in <module>\n    TORCHVISION_AVAILABLE = importlib.util.find_spec(\"torchvision\") is not None\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib.util>\", line 109, in find_spec\nValueError: torchvision.__spec__ is not set\n=============================== warnings summary ===============================\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_authentication_tool_instantiation\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_token_validation_tool_instantiation\ntest/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_rate_limit_tool_instantiation\n  /home/barberb/laion-embeddings-1/src/mcp_server/tool_registry.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n    self.created_at = datetime.utcnow()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED test/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_vector_store_tool_with_parameters\nFAILED test/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_tool_registry_functionality\nFAILED test/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_expected_tool_count\nFAILED test/test_mcp_tools_comprehensive.py::TestMCPToolsIntegration::test_mcp_server_cli_validation\nFAILED test/test_mcp_tools_comprehensive.py::test_mcp_tools_comprehensive - E...\nERROR test/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_all_tools_listed\nERROR test/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_tools_have_valid_schemas\nERROR test/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_mcp_protocol_methods\nERROR test/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_tool_execution_basic\nERROR test/test_mcp_tools_comprehensive.py::TestMCPToolsComprehensive::test_error_handling\n============== 5 failed, 8 passed, 3 warnings, 5 errors in 20.31s ==============\n",
        "stderr": ""
      }
    },
    "mcp_server": {
      "status": "failed",
      "details": {
        "error": "Exit code 1",
        "stderr": "2025-06-07 00:15:07,555 - __main__ - INFO - \ud83d\udd25 Starting LAION Embeddings MCP Server...\n2025-06-07 00:15:07,555 - __main__ - INFO - \ud83d\ude80 Initializing LAION Embeddings MCP Server...\n2025-06-07 00:15:08,534 - datasets - INFO - PyTorch version 2.7.1 available.\n2025-06-07 00:15:11,217 - faiss.loader - INFO - Loading faiss with AVX2 support.\n2025-06-07 00:15:11,242 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n2025-06-07 00:15:11,250 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n2025-06-07 00:15:20,223 - __main__ - ERROR - \u274c Failed to initialize MCP server: No module named 'ipfs_kit_py.ipfs_kit'\n2025-06-07 00:15:20,224 - __main__ - ERROR - Traceback (most recent call last):\n  File \"/home/barberb/laion-embeddings-1/mcp_server.py\", line 46, in initialize\n    from ipfs_embeddings_py.ipfs_embeddings import ipfs_embeddings_py\n  File \"/home/barberb/laion-embeddings-1/ipfs_embeddings_py/__init__.py\", line 1, in <module>\n    from .ipfs_embeddings import ipfs_embeddings_py\n  File \"/home/barberb/laion-embeddings-1/ipfs_embeddings_py/ipfs_embeddings.py\", line 98, in <module>\n    from ipfs_kit_py.ipfs_kit import ipfs_kit\nModuleNotFoundError: No module named 'ipfs_kit_py.ipfs_kit'\n\n2025-06-07 00:15:20,224 - __main__ - ERROR - \u274c Server initialization failed\n",
        "stdout": ""
      }
    },
    "vector_service": {
      "status": "passed",
      "details": {
        "returncode": 0,
        "stdout": "",
        "stderr": ""
      }
    },
    "ipfs_service": {
      "status": "passed",
      "details": {
        "returncode": 0,
        "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.0, pluggy-1.6.0 -- /home/barberb/laion-embeddings-1/.venv/bin/python\ncachedir: .pytest_cache\nbenchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\nrootdir: /home/barberb/laion-embeddings-1\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.1.1, benchmark-5.1.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 15 items\n\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_ipfs_storage_initialization PASSED [  6%]\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_store_vector_shard PASSED [ 13%]\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_retrieve_vector_shard PASSED [ 20%]\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_store_index_manifest PASSED [ 26%]\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_retrieve_index_manifest PASSED [ 33%]\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_ipfs_connection_failure PASSED [ 40%]\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_missing_ipfs_client PASSED [ 46%]\ntest/test_ipfs_vector_service.py::TestDistributedVectorIndex::test_add_vectors_distributed PASSED [ 53%]\ntest/test_ipfs_vector_service.py::TestDistributedVectorIndex::test_search_distributed PASSED [ 60%]\ntest/test_ipfs_vector_service.py::TestDistributedVectorIndex::test_load_from_manifest PASSED [ 66%]\ntest/test_ipfs_vector_service.py::TestDistributedVectorIndex::test_shard_creation PASSED [ 73%]\ntest/test_ipfs_vector_service.py::TestDistributedVectorIndex::test_error_handling_in_search PASSED [ 80%]\ntest/test_ipfs_vector_service.py::TestIPFSIntegration::test_round_trip_vector_storage PASSED [ 86%]\ntest/test_ipfs_vector_service.py::TestIPFSIntegration::test_manifest_consistency PASSED [ 93%]\ntest/test_ipfs_vector_service.py::TestIPFSPerformance::test_large_shard_storage PASSED [100%]\n\n=============================== warnings summary ===============================\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_ipfs_storage_initialization\n  /home/barberb/laion-embeddings-1/.venv/lib/python3.12/site-packages/faiss/loader.py:49: DeprecationWarning: numpy.core._multiarray_umath is deprecated and has been renamed to numpy._core._multiarray_umath. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core._multiarray_umath.__cpu_features__.\n    from numpy.core._multiarray_umath import __cpu_features__\n\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_ipfs_storage_initialization\n  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_ipfs_storage_initialization\n  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n\ntest/test_ipfs_vector_service.py::TestIPFSVectorStorage::test_ipfs_storage_initialization\n  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n======================== 15 passed, 4 warnings in 0.41s ========================\n",
        "stderr": ""
      }
    },
    "clustering": {
      "status": "passed",
      "details": {
        "returncode": 0,
        "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.0, pluggy-1.6.0 -- /home/barberb/laion-embeddings-1/.venv/bin/python\ncachedir: .pytest_cache\nbenchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\nrootdir: /home/barberb/laion-embeddings-1\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.1.1, benchmark-5.1.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 19 items\n\ntest/test_clustering_service.py::TestClusterConfig::test_cluster_config_defaults PASSED [  5%]\ntest/test_clustering_service.py::TestClusterConfig::test_cluster_config_custom PASSED [ 10%]\ntest/test_clustering_service.py::TestVectorClusterer::test_clusterer_initialization PASSED [ 15%]\ntest/test_clustering_service.py::TestVectorClusterer::test_kmeans_clustering PASSED [ 21%]\ntest/test_clustering_service.py::TestVectorClusterer::test_hierarchical_clustering PASSED [ 26%]\ntest/test_clustering_service.py::TestVectorClusterer::test_predict_cluster PASSED [ 31%]\ntest/test_clustering_service.py::TestVectorClusterer::test_predict_before_fit_error PASSED [ 36%]\ntest/test_clustering_service.py::TestVectorClusterer::test_get_cluster_stats PASSED [ 42%]\ntest/test_clustering_service.py::TestVectorClusterer::test_sklearn_import_error PASSED [ 47%]\ntest/test_clustering_service.py::TestSmartShardingService::test_create_clustered_shards PASSED [ 52%]\ntest/test_clustering_service.py::TestSmartShardingService::test_search_clustered_shards PASSED [ 57%]\ntest/test_clustering_service.py::TestSmartShardingService::test_search_with_limited_clusters PASSED [ 63%]\ntest/test_clustering_service.py::TestSmartShardingService::test_search_without_clustering_info PASSED [ 68%]\ntest/test_clustering_service.py::TestSmartShardingService::test_error_handling_in_clustered_search PASSED [ 73%]\ntest/test_clustering_service.py::TestClusteringIntegration::test_end_to_end_clustering_workflow PASSED [ 78%]\ntest/test_clustering_service.py::TestClusteringIntegration::test_cluster_quality_metrics PASSED [ 84%]\ntest/test_clustering_service.py::TestClusteringIntegration::test_adaptive_cluster_search PASSED [ 89%]\ntest/test_clustering_service.py::TestClusteringPerformance::test_large_dataset_clustering PASSED [ 94%]\ntest/test_clustering_service.py::TestClusteringPerformance::test_concurrent_shard_operations PASSED [100%]\n\n=============================== warnings summary ===============================\ntest/test_clustering_service.py::TestClusterConfig::test_cluster_config_defaults\n  /home/barberb/laion-embeddings-1/.venv/lib/python3.12/site-packages/faiss/loader.py:49: DeprecationWarning: numpy.core._multiarray_umath is deprecated and has been renamed to numpy._core._multiarray_umath. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core._multiarray_umath.__cpu_features__.\n    from numpy.core._multiarray_umath import __cpu_features__\n\ntest/test_clustering_service.py::TestClusterConfig::test_cluster_config_defaults\n  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n\ntest/test_clustering_service.py::TestClusterConfig::test_cluster_config_defaults\n  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n\ntest/test_clustering_service.py::TestClusterConfig::test_cluster_config_defaults\n  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n======================== 19 passed, 4 warnings in 2.40s ========================\n",
        "stderr": ""
      }
    },
    "integration": {
      "status": "failed",
      "details": {
        "returncode": 4,
        "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.0, pluggy-1.6.0 -- /home/barberb/laion-embeddings-1/.venv/bin/python\ncachedir: .pytest_cache\nbenchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\nrootdir: /home/barberb/laion-embeddings-1\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.1.1, benchmark-5.1.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items\n\n============================ no tests ran in 0.01s =============================\n",
        "stderr": "ERROR: file or directory not found: test/test_integration_standalone.py\n\n"
      }
    },
    "imports": {
      "status": "failed",
      "details": {
        "returncode": 4,
        "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.0, pluggy-1.6.0 -- /home/barberb/laion-embeddings-1/.venv/bin/python\ncachedir: .pytest_cache\nbenchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\nrootdir: /home/barberb/laion-embeddings-1\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.1.1, benchmark-5.1.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items\n\n============================ no tests ran in 0.02s =============================\n",
        "stderr": "ERROR: file or directory not found: test/test_basic_imports.py\n\n"
      }
    },
    "dependencies": {
      "status": "failed",
      "details": {
        "returncode": 4,
        "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.0, pluggy-1.6.0 -- /home/barberb/laion-embeddings-1/.venv/bin/python\ncachedir: .pytest_cache\nbenchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\nrootdir: /home/barberb/laion-embeddings-1\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.1.1, benchmark-5.1.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items\n\n============================ no tests ran in 0.01s =============================\n",
        "stderr": "ERROR: file or directory not found: test/test_service_dependencies.py\n\n"
      }
    }
  }
}